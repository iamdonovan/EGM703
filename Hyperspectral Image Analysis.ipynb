{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "residential-harrison",
   "metadata": {},
   "source": [
    "# EGM703 - Week 2 Practical: Hyperspectral image analysis\n",
    "\n",
    "## Overview\n",
    "In the lectures and reading this week, you've learned about hyperspectral remote sensing and a number of different methods for analyzing hyperspectral data. In this practical, we'll gain some experience working with hyperspectral data, using a few examples written in python.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Open and view data using xarray\n",
    "- Perform atmospheric correction using dark object subtraction\n",
    "- Use spectral angle matching to compare spectral signatures and identify surfaces\n",
    "- Gain some familiarity with Spectral Python (SPy), a python package for analyzing hyperspectral images.\n",
    "\n",
    "## Data provided\n",
    "In the `data` folder, you should have the following files:\n",
    "\n",
    "- solar_spectra.csv\n",
    "- spectral_library.csv\n",
    "\n",
    "You'll need to download the hyperspectral data from Blackboard, or from the Google Drive link [here](https://drive.google.com/file/d/18EHJpSbkbARJ2Rt6NndBSPe9SlcYr_iO/view?usp=sharing) - be sure to save it to the `data` folder.\n",
    "\n",
    "## 1 Getting started\n",
    "As always, we need to run the following cell to import the necessary libraries, as well as define a couple of functions that we'll use for displaying the image(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "from scipy.interpolate import interp1d\n",
    "import spectral as spy\n",
    "\n",
    "\n",
    "def plot_rgb(ax, ds, bands, crs, variable='radiance'):\n",
    "    \"\"\"\n",
    "    Plot an RGB composite of an image using the provided bands.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax - a matplotlib Axes object\n",
    "    ds - the xarray DataSet with a 'radiance' variable representing the image\n",
    "    bands - a list of the three bands to display, in R, G, B order\n",
    "    crs - a CRS object to pass to ax.imshow()\n",
    "    variable - which variable from ds to show (default: radiance)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ax - the matplotlib Axes object\n",
    "    \"\"\"\n",
    "    dispimg = []\n",
    "    for b in bands:\n",
    "        this_band = ds[variable].loc[b].values\n",
    "        this_band = percentile_stretch(this_band)\n",
    "        dispimg.append(this_band)\n",
    "    dispimg = np.array(dispimg)\n",
    "    ax.imshow(dispimg.transpose([1, 2, 0]), transform=crs, extent=[ds.x.min(), ds.x.max(), ds.y.min(), ds.y.max()])\n",
    "    return ax\n",
    "    \n",
    "\n",
    "def percentile_stretch(image, pmin=0., pmax=100.):\n",
    "    \"\"\"\n",
    "    Apply a linear percentile stretch to an image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image - the input image\n",
    "    pmin - the minimum percentile to use in the stretch\n",
    "    pmax - the maximum percentile to use in the stretch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stretched - the stretched image band\n",
    "    \"\"\"\n",
    "    # here, we make sure that pmin < pmax, and that they are between 0, 100\n",
    "    if not 0 <= pmin < pmax <= 100:\n",
    "        raise ValueError('0 <= pmin < pmax <= 100')\n",
    "    # here, we make sure that the image is only 2-dimensional\n",
    "    if not image.ndim == 2:\n",
    "        raise ValueError('Image can only have two dimensions (row, column)')\n",
    "    \n",
    "    minval = np.percentile(image[image > 0], pmin)\n",
    "    maxval = np.percentile(image[image > 0], pmax)\n",
    "    \n",
    "    stretched = (image - minval) / (maxval - minval) # stretch the image to 0, 1\n",
    "    stretched[image < minval] = 0 # set anything less than minval to the new minimum, 0.\n",
    "    stretched[image > maxval] = 1 # set anything greater than maxval to the new maximum, 1.\n",
    "    \n",
    "    return stretched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-pencil",
   "metadata": {},
   "source": [
    "### 1.1 Loading the data\n",
    "\n",
    "The image we'll be using in this practical is an EO-1 Hyperion image, acquired 22 June 2003. The images are terrain-corrected and radiometrically calibrated by the USGS. I have combined them into a single NetCDF file, re-scaled the values to radiance, and removed any bands that don't contain data. For more information about Hyperion images, including the different processing levels, see this [USGS link](https://www.usgs.gov/centers/eros/science/usgs-eros-archive-earth-observing-one-eo-1-hyperion).\n",
    "\n",
    "`xarray` doesn't automatically read all of the file from the disk. This means that once the image is opened, we also need to load the data using `Dataset.load()` - this is mostly so that we don't have to read the data from the disk every time we want to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data/EO1H0380352003173110PF.nc')\n",
    "xmin, ymin, xmax, ymax = ds.x.min(), ds.y.min(), ds.x.max(), ds.y.max()\n",
    "\n",
    "ds.load() # this will load the entire image into memory - it may take a minute!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-zoning",
   "metadata": {},
   "source": [
    "### 1.2 Selecting bands using xarray\n",
    "\n",
    "Once we have the data loaded, we can start to explore. In the output above, you should see the different coordinates - `band`, `x`, and `y`. You should also see the following data variables:\n",
    "\n",
    "- `crs`: this is the CRS variable that tells a GIS software how to display the images\n",
    "- `radiance`: these are the radiance values\n",
    "- `wavelength`: this is the wavelength corresponding to each band.\n",
    "\n",
    "Note, for example, that `radiance` has three dimensions: `band`, `y`, and `x`, while `wavelength` has only one: `band`. This also tells the index order for a given value in the `radiance` array: `ds['radiance'][0, 1000, 500]` corresponds to the first band, the 1001st row, and the 501st column.\n",
    "\n",
    "This can be a little bit confusing here - the first band (index 0) is actually Hyperion Band 8 (because Bands 1-7 have no data). Rather than selecting by the array index, we can select by the `coordinate` - in this case, we would use the actual Hyperion band number. The following should give us information about Band 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['radiance'].loc[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-impression",
   "metadata": {},
   "source": [
    "This returns the Band 8 radiance as a `DataArray` object - this is an array with 3381 rows and 1121 columns. In addition to the associated coordinates, you can also see that there are a number of attributes for the object: the units are W m<sup>-2</sup> sr<sup>-1</sup> Âµm<sup>-1</sup>, indicating that this is a spectral radiance. The `grid_mapping` attribute is something that tells our GIS software where to look to get the georeferencing information about the image - in this case, it's the `crs` variable.\n",
    "\n",
    "### 1.3 Displaying a single band\n",
    "Because these are arrays, we can display them using `matplotlib`'s `imshow()` method. We'll have a look at Band 8 to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "myCRS = ccrs.UTM(12)\n",
    "\n",
    "plt.subplots(1, 1, subplot_kw=dict(projection=myCRS))\n",
    "plt.imshow(ds['radiance'].loc[8], cmap='gray', transform=ccrs.UTM(12), extent=[xmin, xmax, ymin, ymax])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-asthma",
   "metadata": {},
   "source": [
    "You can zoom in the image above to see the different features in the scene - you should see a large lake (Grand Wash) around halfway down the image, along with a large river just South of the lake (the Colorado River). In the Northern part of the image, you should also be able to see a number of canyons and cliffs - the geology in this area, as we'll see later on, is primarily sandstone/sedimentary rocks with some volcanics thrown in for flavor.\n",
    "\n",
    "### 1.4 Displaying an RGB composite\n",
    "\n",
    "To get a bit better overview, we can display an RGB composite image using bands 31 (), 20 (), and 11 (). Because hyperspectral images have much narrower bands than the images we've used in the past, we can actually choose a number of different combinations here and still have a similar picture - feel free to try out different band combinations. For a list of the bands and wavelengths, you can see **this file** in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_fig, ax = plt.subplots(1, 1, figsize=(6, 10), subplot_kw=dict(projection=myCRS))\n",
    "\n",
    "ax = plot_rgb(ax, ds, [31, 20, 11], myCRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-tongue",
   "metadata": {},
   "source": [
    "### 1.4 Plotting spectral curves\n",
    "We can also see how the value of a single pixel varies by band, or wavelength. `xarray` provides two main ways to access these - we can either use the image index, or using the `sel()` method. In the first case, we'll see how to do this using the index. The next cell will plot the value of the radiance as a function of wavelength for the pixel in row 1950, column 500 (somewhere in the middle of the lake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ds['wavelength'], ds['radiance'][:, 1950, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-asthma",
   "metadata": {},
   "source": [
    "Notice how the radiance is significantly higher in the visible wavelengths (400-700 nm), dropping off substantially above about 600 or so nm - this indicates high radiance values in blue and green wavelengths, and significantly lower values in red and infrared wavelengths - much like we would expect for liquid water.\n",
    "\n",
    "Row 1950, column 500 corresponds to `x`, `y` coordinates of (229800, 4011300). Rather than selecting the image coordinate, we can also select using the `x`,`y` coordinates for the image using the `sel` method. The following should then create the same plot as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = ds['radiance'].sel(x=229800, y=4011300, method='nearest')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ds['wavelength'], spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-corner",
   "metadata": {},
   "source": [
    "## 2 Atmospheric Correction\n",
    "Now that we have a few ways to view our data, we can try to correct the radiance values for atmospheric effects. At shorter wavelengths (compared to the thermal infrared) like we have here, we tend to see more atmospheric scattering, increasing significantly towards shorter wavelengths. This is owing to the main ways that electromagnetic radiation scatters off of molecules and particles in the atmosphere, something we'll address in more detail next week.\n",
    "\n",
    "### 2.1 Band Histogram\n",
    "For now, we'll look at atmospheric correction using dark object subtract, something that was introduced (optionally) in the Week 1 practical. This technique is where we take the reflectance of an object (either an optically \"dark\" object, or objects in shadow) and subtract the observed radiance values for that object from the rest of the image. \n",
    "\n",
    "Rather than searching the image for a suitable object, we're going to use a percentile approach - taking the radiance value that is only brighter than 0.5% of the image. First, we can look at the histogram for Band 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data = ds['radiance'].loc[8].values.flatten() # this gives us a vector array that we can pass to plt.hist()\n",
    "\n",
    "_ = plt.hist(data[data > 0], 150) # make sure to take only the values > 0,\n",
    "# otherwise we'll end up with a ton of 0 (nodata) values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-compatibility",
   "metadata": {},
   "source": [
    "Note that there are a few values around 0, but most of the values seem to be between 60 and 120 W m<sup>-2</sup> sr<sup>-1</sup> Âµm<sup>-1</sup>. Based on this, a good estimate for the dark object radiance in this band would be around 60 W m<sup>-2</sup> sr<sup>-1</sup> Âµm<sup>-1</sup>. \n",
    "\n",
    "### 2.2 Finding the dark object value in each band\n",
    "But, we don't want to have to do this by hand for every single band - instead, we'll use `numpy.percentile()` to calculate the value for us (this is, after all, one of the points of doing things programmatically).\n",
    "\n",
    "In this cell, we'll first calculate the values for each next, we'll find the \"dark object\" value for each band using the value for the 0.5 percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_obj = []\n",
    "for b in ds['radiance']:\n",
    "    # by selecting values where the value > 0, we ignore the nodata values\n",
    "    dark_obj.append(np.percentile(b.values[b.values > 0], 0.5))\n",
    "\n",
    "dark_obj = np.array(dark_obj)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ds['wavelength'], dark_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-leadership",
   "metadata": {},
   "source": [
    "Notice how the value drops rapidly once we get through the visible wavelengths - again, this is because we expect to see the amount of atmospheric scattering drop exponentially as wavelength increases. We'll come back to these values again later when we look at individual reflectance curves.\n",
    "\n",
    "## 3 Calculating reflectance\n",
    "\n",
    "### 3.1 Solar radiance\n",
    "Before we calculate reflectance, however, we need to know what the incident radiation is. In the `data` folder is a file called `solar_spectra.csv`, which contains values of extraterrestrial spectral irradiance downloaded from the [National Renewable Energy Laboratory](https://www.nrel.gov/grid/solar-resource/spectra-am1.5.html). These are provided as spectral irradiance in units of W m<sup>-2</sup> nm<sup>-1</sup>, which means that we need to multiply by 1000 (to convert from nm<sup>-1</sup> to Âµm<sup>-1</sup>), and divide by 4&pi; (to get values in W m<sup>-2</sup> sr<sup>-1</sup> Âµm<sup>-1</sup>) in order to compare with our satellite-derived values.\n",
    "\n",
    "We'll also plot the dark object radiance and the radiance for our sample pixel in the lake, in order to see how they compare to the solar irradiance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/solar_spectra.csv')\n",
    "\n",
    "etr_rad = df.etr * 1000 # convert to \"per nanometer\" values\n",
    "\n",
    "plt.figure()\n",
    "# note: in the line below, we divide by 4pi to get the value per steradian, to match with the units\n",
    "plt.plot(df.wavelength, etr_rad / 4 / np.pi, 'b', label='solar irradiance')\n",
    "\n",
    "plt.plot(ds['wavelength'], dark_obj, 'r', label='dark object radiance')\n",
    "plt.plot(ds['wavelength'], spec, 'k', label='measured radiance')\n",
    "plt.xlabel('wavelength (nm)')\n",
    "plt.ylabel('Spectral radiance (W m$^{-2}$ sr$^{-1}$ nm$^{-1}$)')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-collapse",
   "metadata": {},
   "source": [
    "### 3.2 Calculating reflectance using COST\n",
    "\n",
    "Now that we have the dark object radiance for each band and the solar irradiance, we can calculate the reflectance &rho;<sub>&lambda;</sub> using the corrected radiance (_L_<sub>&lambda;</sub> - _L_<sub>dark</sub>) and the solar irradiance (_L_<sub>sun</sub>):\n",
    "\n",
    "![the COST equation](img/cost_eqn.png)\n",
    "\n",
    "Where _d_ is the Earth-Sun distance in Astronomical Units and _&theta;_<sub>z</sub> is the solar zenith angle (to get this, we subtract the sun elevation angle found in the metadata, 65.098308, from 90).\n",
    "\n",
    "For the scene we are using, acquired 22 June 2003, the Earth-Sun distance is 152040710.84 km, or about 1.0163294 AU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_zenith = np.deg2rad(90 - 65.098308) # solar zenith angle converted to radians\n",
    "d = 152040710.84 / 149597870.700 # earth-sun distance converted to astronomical units\n",
    "\n",
    "refl_toa = (spec * d**2 * np.pi) / (etr_rad * np.cos(solar_zenith)**2)\n",
    "refl = ((spec - dark_obj) * d**2 * np.pi) / (etr_rad * np.cos(solar_zenith)**2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ds['wavelength'], refl_toa, label='Top-of-Atmosphere Reflectance')\n",
    "plt.plot(ds['wavelength'], refl, label='Corrected Reflectance')\n",
    "\n",
    "plt.xlabel('wavelength (nm)')\n",
    "plt.ylabel('reflectance')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-creek",
   "metadata": {},
   "source": [
    "Next, we'll define a helper function, `calculate_reflectance`, and apply it to each band's radiance values to get the reflectance in each band. Then, we'll add this variable to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reflectance(radiance, dark, etr, d, theta_z):\n",
    "    return ((radiance - dark) * d**2 * np.pi) / (etr * np.cos(theta_z)**2)\n",
    "\n",
    "reflectances = []\n",
    "for i, band in enumerate(ds['radiance']):\n",
    "    this_refl = calculate_reflectance(band.values, dark_obj[i], etr_rad[i], d, solar_zenith)\n",
    "    this_refl[band.values == 0] = 0 # make sure that values that were 0 stay 0.\n",
    "    this_refl[this_refl < 0] = 0.01 # set negative reflectance to a low value\n",
    "    reflectances.append(this_refl)\n",
    "\n",
    "# add a new variable to the dataset, reflectance, using the values calculated above\n",
    "ds['reflectance'] = xr.DataArray(np.array(reflectances), dims=['band', 'y', 'x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-thousand",
   "metadata": {},
   "source": [
    "Finally, we'll show the RGB image again, side-by-side with the atmospherically-corrected reflectance values. You should notice that the corrected image appears crisper, in addition to being significantly less blue - this is in part due to the increased scattering seen at shorter wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_fig, axs = plt.subplots(1, 2, figsize=(8, 10), subplot_kw=dict(projection=myCRS))\n",
    "\n",
    "axs[0] = plot_rgb(axs[0], ds, [31, 20, 11], myCRS, variable='radiance')\n",
    "axs[1] = plot_rgb(axs[1], ds, [31, 20, 11], myCRS, variable='reflectance')\n",
    "\n",
    "axs[0].set_title('Raw radiance')\n",
    "axs[1].set_title('Atmospherically-corrected reflectance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-chorus",
   "metadata": {},
   "source": [
    "## 4. Spectral Angle Mapping\n",
    "\n",
    "### 4.1 Using a single end member\n",
    "Next up, we'll see how we can calculate the angle between the spectral vector for our example pixel and water. We'll start by loading our spectral library samples, then calculate the spectral angle &alpha; according to the formula:\n",
    "\n",
    "![spectral angle mapping formula](img/sam_eqn.png)\n",
    "\n",
    "For this, we'll use [Spectral Python](https://www.spectralpython.net/) (SPy), a python module for processing hyperspectral data. In addition to spectral angle mapping, SPy also has a number of algorithms that we have discussed, including minimum noise fraction (MNF) and principal component analysis (PCA), and it also includes tools for querying the ECOSTRESS Spectral Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spectral library values that have been re-sampled to the same wavelengths as the EO-1 image.\n",
    "spectral_library = pd.read_csv('data/spectral_library.csv')\n",
    "\n",
    "water_angles = spy.spectral_angles(ds['reflectance'].values.transpose([1, 2, 0]), \n",
    "                                   spectral_library['water'].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-purchase",
   "metadata": {},
   "source": [
    "We can now look at the spectral angle for each pixel - we'll focus on the lake so we can get an idea of how well it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(water_angles[1800:2000, 450:550], vmin=0.5, vmax=0.8)\n",
    "# plt.imshow(angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-stuff",
   "metadata": {},
   "source": [
    "You should see that the water has a comparatively small angle (dark colors), while the land has a typically larger angle - overall, it seems to have worked as we might expect, although it does appear somewhat noisy.\n",
    "\n",
    "With one spectral signature like this example, we could choose a threshold angle for binary classification - pixels with an angle less than the given threshold would be classified as water, and pixels with an angle larger than the given threshold would be classified as 'not water'\n",
    "\n",
    "To see why the results for this example might be somewhat noisy, let's look at the reflectance spectra for a few example pixels from the lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also select a range of pixels using xarray\n",
    "# in this example, we're selecting based on a range of x and y values\n",
    "test_pixels = ds['reflectance'].sel(x=np.linspace(229300, 229600, 10),\n",
    "                                    y=np.linspace(4011800, 4012100, 10), method='nearest')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ds['wavelength'], test_pixels.values.reshape(194, -1), '0.5')\n",
    "\n",
    "plt.plot(ds['wavelength'], spectral_library['water'].values, 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-church",
   "metadata": {},
   "source": [
    "In the example above, we can see that there's quite a bit of noise in our reflectance spectra - this is contributing to the noise we can see in the spectral angle results.\n",
    "\n",
    "### 4.2 Multiple end members\n",
    "\n",
    "With additional reference spectra (end members), we can find which end member each pixel is 'closest' to. With `spectral.spectral_angles()`, we can use as many end members as we have data for. In the next example will do this for all of our reference spectra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectral_library.columns[1:])\n",
    "\n",
    "end_members = spectral_library.columns[1:]\n",
    "\n",
    "# spectral_angles expects the data to have a shape (rows, columns, bands), while our data are (bands, rows, columns)\n",
    "# we also need to transpose our spectral library data so that the rows correspond to each end member\n",
    "all_angles = spy.spectral_angles(ds['reflectance'].values.transpose([1, 2, 0]), \n",
    "                                 spectral_library[end_members].values.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_library[end_members].values.transpose().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-heart",
   "metadata": {},
   "source": [
    "Using `numpy.argmin()`, we can then display an image where the value of each pixel corresponds to the end member with the smallest angle for each pixel - in other words, the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# this will tell us which end member has the smallest spectral angle for each pixel\n",
    "best_match = np.argmin(all_angles, axis=2)\n",
    "\n",
    "# this will set the nodata pixels to 255, so that we can safely ignore them.\n",
    "best_match[ds['reflectance'].loc[8] == 0] = 255\n",
    "\n",
    "plt.imshow(best_match, vmax=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-grammar",
   "metadata": {},
   "source": [
    "Here, we can see that the Lake has a fairly uniform value of 0, indicating that we've done a good job classifying the water. We can also see that in the northern part of the image, we have a number of areas where we've successfully picked out the stands of ponderosa pines (value of 1). While most of the image has been classified as sandstone, we know from the [geologic maps](http://data.azgs.az.gov/geologic-map-of-arizona/) of the area that this isn't completely correct - we should see areas of basalts and other types of bedrock.\n",
    "\n",
    "To get a better classification of the image, we would want to be sure to pick out additional end members, and possibly even include multiple samples - remember that small differences in grain size or chemical composition can have a large impact on the spectral signature.\n",
    "\n",
    "While these results aren't bad for an initial attempt, the image is still relatively noisy. To help reduce the impact of the noise in the data, we might also consider using the minimum noise fraction or a similar data reduction technique.\n",
    "\n",
    "### 4.3 Exporting the results\n",
    "\n",
    "As a last step, we'll export this classified image to a raster file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "\n",
    "with rio.open('EO1H0380352003173110PF_RGB.tif', 'r') as src:\n",
    "    profile = src.profile\n",
    "    profile.update({'dtype': np.uint8, 'count': 1, 'nodata': 255})\n",
    "    with rio.open('sam_classification.tif', 'w', **profile) as dst:\n",
    "        dst.write(best_match.astype(np.uint8), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-corporation",
   "metadata": {},
   "source": [
    "You should be able to load the classified raster in a GIS software such as ArcGIS Pro or QGIS, and calculate the area and percent coverage of each end member in the image.\n",
    "\n",
    "That will wrap us up for this week's practical - as mentioned, there are a number of additional algorithms available from `spectral` - for a full list, check out the full [API documentation](https://www.spectralpython.net/class_func_ref.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
